{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import itertools\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pyquery import PyQuery as pq\n",
    "dropbox = \"C:\\Users\\mkkes_000\\Dropbox\\Indiastuff\\OutputTables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_elections = [\"andhra2014\",\"arunachal2014\",\"bihar2015\",\"chhattisgarh2013\",\"delhi2015\",\"gujarat2012\",\"haryana2014\",\"hp2012\",\"jk2014\",\n",
    "              \"jharkhand2014\",\"karnataka2013\",\"kerala2011\",\"mp2013\",\"maharashtra2014\",\"manipur2012\",\"odisha2014\",\"pb2012\",\n",
    "              \"rajasthan2013\",\"sikkim2014\",\"tamilnadu2011\",\"telangana2014\",\"tripura2013\",\"utt2012\",\"up2012\",\"westbengal2011\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For each election we take the district name and the constituency names.\n",
    "# We want to create a tuple for each constituency\n",
    "# of structure (name, district, id_number)\n",
    "\n",
    "def only_highest(l):\n",
    "    list_highest = []\n",
    "    for i in range(0,len(l)-1):\n",
    "        if l[i+1]>l[i]:\n",
    "            list_highest = list_highest\n",
    "        else:\n",
    "            list_highest.append(l[i])\n",
    "    return list_highest\n",
    "\n",
    "def constituency_tuple(e):\n",
    "    long_link = \"http://myneta.info/\"+e+\"/\"\n",
    "    source=requests.get(long_link)\n",
    "    time.sleep(1)\n",
    "    tree= BeautifulSoup(source.text,\"html.parser\")\n",
    "    names_districts = [x.get_text().strip().title() for x in tree.findAll(\"h5\", { \"class\" : \"title\" })]\n",
    "    list_const = tree.findAll(\"div\", { \"class\" : \"items\" })[1:]\n",
    "    names_constituencies = [x.get_text().split(\":\")[1].strip().title().split(\"*\")[0] for x in list_const]\n",
    "    link_constit = [x.find(\"a\").get(\"href\") for x in tree.findAll(\"div\", { \"class\" : \"items\" })[1:]]\n",
    "    id_constit = [x.split(\"=\")[2] for x in link_constit]\n",
    "    # Only taking the first election\n",
    "    n_constituencies = only_highest([int(x.get_text().split(\":\")[0].split(\"(\")[0])\n",
    "                                 for x in tree.findAll(\"div\", { \"class\" : \"items\" })[1:]])\n",
    "    list_districts = list(itertools.chain(\n",
    "                    *[[names_districts[i]]*n_constituencies[i] for i in range(0,len(names_districts)-1)]))\n",
    "    return zip(names_constituencies,list_districts,id_constit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Creates a dictionary state:[all constituencies] \n",
    "#where all constituency is a tuple (constituency,district,id of constituency)\n",
    "all_elections = {el:constituency_tuple(el) for el in list_elections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just building dictionaries to store info more efficiently\n",
    "def store_dict(el):\n",
    "    dtuple = all_elections[el]\n",
    "    constituency_district =  {l[2]:l[1] for l in dtuple}\n",
    "    return constituency_district\n",
    "\n",
    "def store_dict2(el):\n",
    "    dtuple = all_elections[el]\n",
    "    constituency_code =  {l[2]:l[0] for l in dtuple}\n",
    "    return constituency_code\n",
    "\n",
    "def store_constituencies(el):\n",
    "    dtuple = all_elections[el]\n",
    "    allcodes =  [l[2] for l in dtuple]\n",
    "    return allcodes\n",
    "\n",
    "election_constituencies = {}\n",
    "allconst_dist = {}\n",
    "allconst_codes = {}\n",
    "for el in list_elections:\n",
    "    allconst_dist[el] = store_dict(el)\n",
    "    allconst_codes[el] = store_dict2(el)\n",
    "    election_constituencies[el] = store_constituencies(el)\n",
    "\n",
    "with open(\"tempdata/const_district\",'w') as js_file:\n",
    "    json.dump(allconst_dist,js_file)\n",
    "with open(\"tempdata/const_codes\",'w') as js_file:\n",
    "    json.dump(allconst_codes,js_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cand_info(line):\n",
    "    all_info = line.findAll(\"td\")\n",
    "    name = all_info[0].get_text()\n",
    "    cand_link = all_info[0].find(\"a\").get(\"href\")\n",
    "    cand_id = cand_link.split(\"=\")[1]\n",
    "    try:\n",
    "        age = int(all_info[1].get_text())\n",
    "    except TypeError:\n",
    "        age = np.nan\n",
    "    sex = all_info[2].get_text()\n",
    "    party = all_info[3].get_text()\n",
    "    serious_crim_yn = all_info[4].get_text()\n",
    "    try:\n",
    "        crim = int(all_info[5].get_text())\n",
    "        serious_crim_num = int(all_info[6].get_text())\n",
    "    except ValueError:\n",
    "        crim = np.nan\n",
    "        serious_crim_num = np.nan\n",
    "    if all_info[7].get_text()==\"\":\n",
    "        crim_types = []\n",
    "    else:\n",
    "        crim_types = [x.get_text() for x in all_info[7].findAll(\"span\")]\n",
    "    educ = all_info[8].get_text()\n",
    "    list_assets = [int(all_info[i].get_text().split(\"~\")[0].replace(',', '')) for i in [9,10,11,12]]\n",
    "    #mov_assets = all_info[9].get_text().split(\"~\")[0]\n",
    "    #immov_assets = all_info[10].get_text().split(\"~\")[0]\n",
    "    #total_assets = all_info[11].get_text().split(\"~\")[0]\n",
    "    #total_liab = all_info[12].get_text().split(\"~\")[0]\n",
    "    pan = all_info[13].get_text()\n",
    "    list_all = [cand_link,cand_id,name,age,sex,party,serious_crim_yn,crim,serious_crim_num,crim_types,educ]+list_assets+[pan]\n",
    "    return list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dictionary of dictionaries in the format:\n",
    "# {election:{constituency_id:detailed constituency page}}\n",
    "def get_page(c,election):\n",
    "    # Check if URL has already been visited.\n",
    "    url_error = []\n",
    "    if (c not in urlcache) or (urlcache[c]==1) or (urlcache[c]==2):\n",
    "        time.sleep(.5)\n",
    "        steps = len(urlcache)\n",
    "        if 100*int(steps/100)==steps:\n",
    "            print steps # This counter tells us how many links were downloaded at every 100 mark\n",
    "        # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n",
    "        # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n",
    "        try:\n",
    "            link_disc = \"http://myneta.info/\"+election+\"/comparisonchart.php?constituency_id=\"+c\n",
    "            c_page=requests.get(link_disc)\n",
    "            if c_page.status_code == 200:\n",
    "                urlcache[c] = c_page.text\n",
    "            else:\n",
    "                urlcache[c] = 1\n",
    "        except:\n",
    "            urlcache[c] = 2\n",
    "            url_error.append(c)\n",
    "            print c\n",
    "    return urlcache[c]\n",
    "\n",
    "def get_info_constituency(election):\n",
    "    const_list = election_constituencies[election]\n",
    "    print \"\\n\", election, \" of length\", len(const_list), \":\",\n",
    "    dict_store = {}\n",
    "    for c in const_list:\n",
    "        link_disc = \"http://myneta.info/\"+election+\"/comparisonchart.php?constituency_id=\"+c\n",
    "        time.sleep(.5)\n",
    "        c_page=requests.get(link_disc)\n",
    "        dict_store[c] = c_page\n",
    "        if len(dict_store)%100==0 or len(dict_store)==len(const_list):\n",
    "            print len(dict_store),\n",
    "    return dict_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# The structure is the following:\n",
    "# List of dictionaries {state:{constituency:page}}\n",
    "for el in list_elections:\n",
    "    if el in dict_all_pages:\n",
    "        if len(dict_all_pages[el]) == len(election_constituencies[el]):\n",
    "            print \"done for:\", el, \"length:\", len(dict_all_pages[el])\n",
    "        else:\n",
    "            print \"starting again for \", el\n",
    "    else:\n",
    "        dict_all_pages[el] = get_info_constituency(el) #careful, very long call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The all_const function is at the constituency level\n",
    "# Taking a constituency in a given state, it spits out a list of candidates with their info\n",
    "def all_candidates(c,election):\n",
    "    dict_c = dict_all_pages[election]\n",
    "    const_page = dict_c[c]\n",
    "    tree = BeautifulSoup(const_page.text,\"html.parser\")\n",
    "    cand_lines = tree.findAll(\"tr\")[1:]\n",
    "    constituency = allconst_codes[election][c]\n",
    "    district = allconst_dist[election][c]\n",
    "    titles = [\"election\",\"c\",\"constituency\",\"district\",\"link\",\"id\"]+[x.get_text() for x in tree.findAll(\"th\")]\n",
    "    info_candidates = [dict(zip(titles,[election,c,constituency,district]+cand_info(r))) for r in cand_lines]\n",
    "    return info_candidates\n",
    "def state_mapper(el):\n",
    "    all_cand_state = [all_candidates(c_line[2],el) for c_line in all_elections[el]]\n",
    "    return all_cand_state\n",
    "## Takes one of the elements of the all_election_final dictionary\n",
    "## And spits out a dataframe with all the columns. We can then stack them\n",
    "## at a later stage\n",
    "def reducer(el):\n",
    "    list_const = all_elections_final[el]\n",
    "    large_list = []\n",
    "    for cons in list_const:\n",
    "        large_list.extend(cons)\n",
    "    df = pd.DataFrame(large_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_elections_dict = {}\n",
    "for e in list_elections:\n",
    "    all_elections_dict[e]=state_mapper(e)\n",
    "    print e,\n",
    "    all_elections_final = all_elections_dict.copy()\n",
    "df_all = pd.DataFrame()\n",
    "for el in list_elections:\n",
    "    df = reducer(el)\n",
    "    df_all = df_all.append(df)\n",
    "df_fin = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## finding winners ##\n",
    "winners_dict = {}\n",
    "for el in list_elections:\n",
    "    link_winners = \"http://myneta.info/\"+el+\"/index.php?action=summary&subAction=winner_analyzed&sort=candidate#summary\"\n",
    "    wins = requests.get(link_winners)\n",
    "    time.sleep(1)\n",
    "    tree = BeautifulSoup(wins.text,\"html.parser\")\n",
    "    tables = tree.findAll(\"table\")\n",
    "    list_ids = [x.find(\"a\").get(\"href\").split(\"=\")[1] for x in tables[2].findAll(\"tr\")[2:]]\n",
    "    winners_dict[el]=list_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparison_clean(line,el):\n",
    "    id2 = line.find(\"a\").get(\"href\").split(\"=\")[3].split(\"&\")[0] #id2 is the latest\n",
    "    # We are going to match it to the df and add the new id + old info\n",
    "    id1 = line.find(\"a\").get(\"href\").split(\"=\")[4]\n",
    "    elements = [x.get_text() for x in line.findAll(\"td\")]\n",
    "    name = elements[1].split(\"(\")[0].strip()\n",
    "    new_assets = elements[2].split(u'\\xa0')[0].replace(\",\",\"\")\n",
    "    old_assets = elements[3].split(u'\\xa0')[0].replace(\",\",\"\")\n",
    "    increase = elements[4].split(u'\\xa0')[0].replace(\",\",\"\")\n",
    "    try:\n",
    "        increase_pct = float(elements[5].replace('%',''))\n",
    "    except ValueError:\n",
    "        increase_pct = elements[5].replace('%','')\n",
    "    list_repeat = [el,id2,id1,name,new_assets,old_assets,increase,increase_pct]\n",
    "    titles = ['election','new_id','old_id','name2','assets new','assets old','increase','pct change']\n",
    "    dict_repeat = dict(zip(titles,list_repeat))\n",
    "    return dict_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_repeat = pd.DataFrame()\n",
    "for el in list_elections:\n",
    "    link_repeat = \"http://myneta.info/\"+el+\"/index.php?action=recontestAssetsComparison\"\n",
    "    page = requests.get(link_repeat)\n",
    "    tree = BeautifulSoup(page.text,\"html.parser\")\n",
    "    tables = tree.findAll(\"table\")\n",
    "    all_lines = tables[2].findAll(\"tr\")[2:]\n",
    "    info_repeat = [comparison_clean(l,el) for l in all_lines if len(l)==7]\n",
    "    df_state = pd.DataFrame(info_repeat)\n",
    "    df_repeat = df_repeat.append(df_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for el,list_id in winners_dict.items():\n",
    "    df_fin['win'][df_fin.election==el]= df_fin.apply(lambda row: (row['election']==el)==True and (row['id'] in list_id)==True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fin.to_csv(\"tempdata\\df_all_regional_elections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_tsform(el,c_id):\n",
    "    url_link = \"http://myneta.info/%s/candidate.php?candidate_id=%s\" %(el,c_id)\n",
    "    return url_link\n",
    "def url_split(url_link):\n",
    "    return url_link.split(\"/\")[3], url_link.split(\"=\")[1]\n",
    "def get_page(el,c_id):\n",
    "    # Check if URL has already been visited.\n",
    "    url = (el,c_id)\n",
    "    url_error = []\n",
    "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "        time.sleep(.5)\n",
    "        try:\n",
    "            r = requests.get(url_tsform(el,c_id))\n",
    "            if r.status_code == 200:\n",
    "                urlcache[url] = r.text\n",
    "            else:\n",
    "                urlcache[url] = 1\n",
    "        except:\n",
    "            urlcache[url] = 2\n",
    "            url_error.append(url)\n",
    "            print \"error with:\", url\n",
    "    return urlcache[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates a giant dictionary\n",
    "#dict_id2 = {}\n",
    "steps = len(dict_id2)\n",
    "print \"initial length: \", steps\n",
    "for row in df_fin.itertuples():\n",
    "    if (row[19],row[20]) in dict_id2:\n",
    "        pass\n",
    "    else:\n",
    "        dict_id2[(row[19],row[20])] = get_page(row[19],row[20])\n",
    "        steps = len(dict_id2)\n",
    "        if steps % 1000 ==0:\n",
    "            print steps,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_clean = {url_tsform(k[0],k[1]):v for k,v in dict_id2.iteritems() if v!=2}\n",
    "print len(dict_clean), len(dict_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tempdata/largedict.json', 'w') as f:\n",
    "     json.dump(dict_clean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now create complete information file on each 37000 candidates \n",
    "# On assets, literacy, income (when available) and criminality\n",
    "# As well as whether they have repeated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Re open the dict\n",
    "with open('tempdata/largedict.json', 'r') as f:\n",
    "     dict_start = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the assets for each candidate #\n",
    "def movable_assets(info,url):\n",
    "    soup=BeautifulSoup(info[url],\"html.parser\")\n",
    "    table = soup.find(\"a\", attrs={\"name\": \"movable_assets\"}).find(\"table\")\n",
    "    madict={}\n",
    "    madict[\"url\"]=url\n",
    "    \n",
    "    if url[19:21]==\"lo\":\n",
    "        madict['year']=url[len('http://myneta.info/loksabha'): 4 + len('http://myneta.info/loksabha')]\n",
    "    else:\n",
    "        madict['year']=url[len('http://myneta.info/ls'): 4 + len('http://myneta.info/ls')]\n",
    "   \n",
    "    for r in table.findAll('td'):\n",
    "        try:\n",
    "            if r.get_text()==\"i\":\n",
    "                madict[\"Cash\"] = r.findNext('b').get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "\n",
    "            if r.get_text()==\"ii\":\n",
    "                madict[\"Deposits in Banks, Financial Institutions and Non-Banking Financial Companies\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "            if r.get_text()==\"iii\":\n",
    "                madict[\"Bonds, Debentures and Shares in companies\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "                             # use different tag and attribute to pick up text. Limit specifies # of iteration for findall method\n",
    "            if r.get_text()==\"iv\":\n",
    "                madict[\"NSS, Postal Savings etc\"] = r.findAllNext('b',limit=4)[1].get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "                madict[\"LIC or other insurance Policies\"] = r.findAllNext('b',limit=4)[3].get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "           \n",
    "            \n",
    "            if url[19:25]==\"ls2014\":\n",
    "                \n",
    "                if r.get_text()==\"v\":\n",
    "                    madict[\"Personal loans/advance given\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"vi\":\n",
    "                    madict[\"Motor Vehicles (details of make, etc.)\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"vii\":\n",
    "                    madict[\"Jewellery (give details weight value)\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"viii\":\n",
    "                    madict[\"Other assets, such as values of claims / interests\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"Gross Total Value (as per Affidavit)\":\n",
    "                    madict[\"Gross Total Value (as per Affidavit)\"] = r.findAllNext('b',limit=1)[0].get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"Totals (Calculated as Sum of Values)\":\n",
    "                    madict[\"Totals (Calculated as Sum of Values)\"] = r.findAllNext('b',limit=6)[5].get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "                    \n",
    "                madict[\"TOTAL_MOVABLE_ASSESTS\"]=  int(madict[\"Totals (Calculated as Sum of Values)\"].replace(',','') )\n",
    "      \n",
    "            else:\n",
    "                \n",
    "                if r.get_text()==\"v\":\n",
    "                    madict[\"Motor Vehicles (details of make, etc.)\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"vi\":\n",
    "                    madict[\"Jewellery (give details weight value)\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"vii\":\n",
    "                    madict[\"Other assets, such as values of claims / interests\"] = r.findNext('b').get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "\n",
    "                if r.get_text()==\"Totals\":\n",
    "                    madict[\"Totals (Calculated as Sum of Values)\"] = r.findAllNext('b', limit=6)[5].get_text().encode('ascii', 'ignore').replace(\"Rs\",\"\").strip()\n",
    "        \n",
    "                if madict[\"LIC or other insurance Policies\"]==\"Nil\":\n",
    "                    madict[\"TOTAL_MOVABLE_ASSETS\"]=  int(madict[\"Totals (Calculated as Sum of Values)\"].replace(',',''))\n",
    "                else:\n",
    "                    madict[\"TOTAL_MOVABLE_ASSETS\"]=  int(madict[\"Totals (Calculated as Sum of Values)\"].replace(',','')) + int(madict[\"LIC or other insurance Policies\"].replace(',',''))\n",
    "        \n",
    "        except:\n",
    "            pass    \n",
    "        \n",
    "\n",
    "    return madict\n",
    "\n",
    "def immovable_assets(info,url):\n",
    "    soup=BeautifulSoup(info[url],\"html.parser\")\n",
    "    table = soup.findAll(\"table\",attrs={\"class\":\"table1\"})[2]\n",
    "    immadict={}\n",
    "    immadict[\"url\"]=url\n",
    "    \n",
    "    for r in table.findAll('td'):\n",
    "        try:\n",
    "            if r.get_text()==\"i\":\n",
    "                immadict[\"Agricultural Land\"] = r.findNext('b').get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "\n",
    "            if r.get_text()==\"ii\":\n",
    "                immadict[\"Non Agricultural Land\"] = r.findNext('b').get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "           \n",
    "            if r.get_text()==\"iii\":\n",
    "                immadict[\"Commercial Buildings\"] = r.findNext('b').get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "\n",
    "            if r.get_text()==\"iv\":\n",
    "                immadict[\"Residential Buildings\"] = r.findNext('b').get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "\n",
    "            if r.get_text()==\"v\":\n",
    "                immadict[\"Others\"] = r.findNext('b').get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "            \n",
    "            if url[19:25]==\"ls2014\":\n",
    "                \n",
    "                if r.get_text()==\"Total Current Market Value of (i) to (v) (as per Affidavit)\":\n",
    "                    immadict[\"Total Current Market Value of (i) to (v) (as per Affidavit)\"] = r.findAllNext('b',limit=1)[0].get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "\n",
    "                if r.get_text()==\"Totals Calculated\":\n",
    "                    immadict[\"TOTAL_IMMOVABLE_ASSETS\"] = r.findAllNext('b',limit=6)[5].get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "            else:\n",
    "                if r.get_text()==\"Totals\":\n",
    "                    immadict[\"TOTAL_IMMOVABLE_ASSETS\"] = r.findAllNext('b',limit=6)[5].get_text().strip().encode('ascii', 'ignore').replace(\"Rs\",\"\")\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return immadict\n",
    "def cleaner(d):\n",
    "    for key in d.keys():\n",
    "        if d[key]=='Nil' or d[key]=='':\n",
    "            d[key]==np.nan\n",
    "        else:\n",
    "            try:\n",
    "                d[key]=float(d[key].replace(',',''))\n",
    "            except:\n",
    "                pass\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000Wall time: 1h 12min 23s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "movassets={}\n",
    "for url in dict_start.keys():\n",
    "    try:\n",
    "        massetdict=cleaner(movable_assets(dict_start,url))\n",
    "        movassets[url] = massetdict\n",
    "        if len(movassets)%1000==0:\n",
    "            print len(movassets),\n",
    "    except TypeError: \n",
    "        print \"error: \",url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000Wall time: 48min 19s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "immovassets={}\n",
    "for url in dict_start.keys():\n",
    "    try:\n",
    "        immov_assetdict=cleaner(immovable_assets(dict_start,url))\n",
    "        immovassets[url] = immov_assetdict\n",
    "        if len(immovassets)%1000==0:\n",
    "            print len(immovassets),\n",
    "    except TypeError: \n",
    "        print \"error: \",url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immov assets: {'Residential Buildings': 500000.0, 'url': u'http://myneta.info/westbengal2011/candidate.php?candidate_id=35', 'Others': 'Nil', 'Commercial Buildings': 'Nil', 'year': u'stbe', 'Non Agricultural Land': 200000.0, 'Agricultural Land': 'Nil'} \n",
      "Movables:  {'Jewellery (give details weight value)': 'Nil', 'url': u'http://myneta.info/westbengal2011/candidate.php?candidate_id=35', 'NSS, Postal Savings etc': 'Nil', 'year': u'stbe', 'Cash': 20000.0, 'Other assets, such as values of claims / interests': 'Nil', 'LIC or other insurance Policies': 'Nil', 'Bonds, Debentures and Shares in companies': 'Nil', 'Deposits in Banks, Financial Institutions and Non-Banking Financial Companies': 17223.0, 'Motor Vehicles (details of make, etc.)': 'Nil'}\n"
     ]
    }
   ],
   "source": [
    "test_key = immovassets.keys()[12]\n",
    "print \"Immov assets:\", immovassets[test_key], \"\\nMovables: \", movassets[test_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tempdata/mov_assets_state.json','w') as f:\n",
    "    json.dump(movassets,f)\n",
    "with open('tempdata/immov_assets_state.json','w') as f:\n",
    "    json.dump(immovassets,f)   \n",
    "\n",
    "df_mov_assets = pd.DataFrame.from_dict(movassets, orient = 'index')\n",
    "df_immov_assets = pd.DataFrame.from_dict(immovassets, orient = 'index')\n",
    "df_assets = pd.merge(df_mov_assets,df_immov_assets, on = 'url')\n",
    "df_assets.drop(['year_x','year_y'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jewellery (give details weight value)</th>\n",
       "      <th>url</th>\n",
       "      <th>NSS, Postal Savings etc</th>\n",
       "      <th>Cash</th>\n",
       "      <th>Other assets, such as values of claims / interests</th>\n",
       "      <th>LIC or other insurance Policies</th>\n",
       "      <th>Bonds, Debentures and Shares in companies</th>\n",
       "      <th>Deposits in Banks, Financial Institutions and Non-Banking Financial Companies</th>\n",
       "      <th>Motor Vehicles (details of make, etc.)</th>\n",
       "      <th>Residential Buildings</th>\n",
       "      <th>Others</th>\n",
       "      <th>Commercial Buildings</th>\n",
       "      <th>Non Agricultural Land</th>\n",
       "      <th>Agricultural Land</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210000</td>\n",
       "      <td>http://myneta.info/andhra2014/candidate.php?ca...</td>\n",
       "      <td>Nil</td>\n",
       "      <td>13000</td>\n",
       "      <td>210000</td>\n",
       "      <td>990000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>11000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>800000</td>\n",
       "      <td>Nil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nil</td>\n",
       "      <td>http://myneta.info/andhra2014/candidate.php?ca...</td>\n",
       "      <td>Nil</td>\n",
       "      <td>15000</td>\n",
       "      <td>150000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>1500</td>\n",
       "      <td>Nil</td>\n",
       "      <td>1200000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>http://myneta.info/andhra2014/candidate.php?ca...</td>\n",
       "      <td>Nil</td>\n",
       "      <td>20000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>0</td>\n",
       "      <td>Nil</td>\n",
       "      <td>110000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54000</td>\n",
       "      <td>http://myneta.info/andhra2014/candidate.php?ca...</td>\n",
       "      <td>Nil</td>\n",
       "      <td>15000</td>\n",
       "      <td>420000</td>\n",
       "      <td>168788</td>\n",
       "      <td>Nil</td>\n",
       "      <td>27130</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nil</td>\n",
       "      <td>http://myneta.info/andhra2014/candidate.php?ca...</td>\n",
       "      <td>Nil</td>\n",
       "      <td>25000</td>\n",
       "      <td>90000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>1000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>200000</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "      <td>Nil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Jewellery (give details weight value)  \\\n",
       "0                                210000   \n",
       "1                                   Nil   \n",
       "2                                     0   \n",
       "3                                 54000   \n",
       "4                                   Nil   \n",
       "\n",
       "                                                 url NSS, Postal Savings etc  \\\n",
       "0  http://myneta.info/andhra2014/candidate.php?ca...                     Nil   \n",
       "1  http://myneta.info/andhra2014/candidate.php?ca...                     Nil   \n",
       "2  http://myneta.info/andhra2014/candidate.php?ca...                     Nil   \n",
       "3  http://myneta.info/andhra2014/candidate.php?ca...                     Nil   \n",
       "4  http://myneta.info/andhra2014/candidate.php?ca...                     Nil   \n",
       "\n",
       "    Cash Other assets, such as values of claims / interests  \\\n",
       "0  13000                                             210000   \n",
       "1  15000                                             150000   \n",
       "2  20000                                                Nil   \n",
       "3  15000                                             420000   \n",
       "4  25000                                              90000   \n",
       "\n",
       "  LIC or other insurance Policies Bonds, Debentures and Shares in companies  \\\n",
       "0                          990000                                       Nil   \n",
       "1                             Nil                                       Nil   \n",
       "2                             Nil                                       Nil   \n",
       "3                          168788                                       Nil   \n",
       "4                             Nil                                       Nil   \n",
       "\n",
       "  Deposits in Banks, Financial Institutions and Non-Banking Financial Companies  \\\n",
       "0                                              11000                              \n",
       "1                                               1500                              \n",
       "2                                                  0                              \n",
       "3                                              27130                              \n",
       "4                                               1000                              \n",
       "\n",
       "  Motor Vehicles (details of make, etc.) Residential Buildings Others  \\\n",
       "0                                    Nil                   Nil    Nil   \n",
       "1                                    Nil               1200000    Nil   \n",
       "2                                    Nil                110000    Nil   \n",
       "3                                    Nil                   Nil    Nil   \n",
       "4                                    Nil                200000    Nil   \n",
       "\n",
       "  Commercial Buildings Non Agricultural Land Agricultural Land  \n",
       "0                  Nil                800000               Nil  \n",
       "1                  Nil                   Nil               Nil  \n",
       "2                  Nil                   Nil                 0  \n",
       "3                  Nil                   Nil               Nil  \n",
       "4                  Nil                   Nil               Nil  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Let's now obtain all the criminal data #\n",
    "def get_crimdata2(year, page_text, url):\n",
    "    \n",
    "    crim_list = []\n",
    "    crim_det_list = []\n",
    "    success = 0\n",
    "    crim_dict = {'year_election': year, 'url': url}\n",
    "    header = ['serial_no','IPC_sections','Other']\n",
    "    \n",
    "    for h3 in pq(page_text)(\"h3\"):\n",
    "        #print h3.text\n",
    "        if h3.text == \"Cases where charges framed\":\n",
    "            framed = pq(h3).next('table')\n",
    "            crim_dict['type'] = 'Framed'\n",
    "            for i,tr in enumerate(pq(framed)('tr')):\n",
    "                crim_dict_copy = crim_dict.copy()\n",
    "                if i > 0:\n",
    "                    for j,child in enumerate(tr.iterchildren()):\n",
    "                        if child.text != '---------':\n",
    "                            crim_dict_copy[header[j]] = child.text\n",
    "                            success = 1\n",
    "                    if success == 1:\n",
    "                        crim_list.append(crim_dict_copy)\n",
    "                        success = 0\n",
    "        if h3.text == \"Cases where Cognizance taken\":\n",
    "            framed = pq(h3).next('table')\n",
    "            crim_dict['type'] = 'Cogniz'\n",
    "            for i,tr in enumerate(pq(framed)('tr')):\n",
    "                crim_dict_copy = crim_dict.copy()\n",
    "                if i > 0:\n",
    "                    for j,child in enumerate(tr.iterchildren()):\n",
    "                        if child.text != '---------':\n",
    "                            crim_dict_copy[header[j]] = child.text\n",
    "                            success = 1\n",
    "                    if success == 1:\n",
    "                        crim_list.append(crim_dict_copy)\n",
    "                        success = 0\n",
    "        if h3.text == \"Cases where convicted\":\n",
    "            framed = pq(h3).next('table')\n",
    "            crim_dict['type'] = 'Convict'\n",
    "            for i,tr in enumerate(pq(framed)('tr')):\n",
    "                crim_dict_copy = crim_dict.copy()\n",
    "                if i > 0:\n",
    "                    for j,child in enumerate(tr.iterchildren()):\n",
    "                        if child.text != '---------':\n",
    "                            crim_dict_copy[header[j]] = child.text\n",
    "                            success = 1\n",
    "                    if success == 1:\n",
    "                        crim_list.append(crim_dict_copy)\n",
    "                        success = 0\n",
    "        \n",
    "        if h3.text == \"Brief Details of IPCs\":\n",
    "            crim_det_dict = {'url': url}\n",
    "            parent = pq(h3).parent()\n",
    "            details = parent.text().split(') ')\n",
    "            \n",
    "            for x in details:\n",
    "                if len(x)>0:\n",
    "                    crim_det_copy = crim_det_dict.copy()\n",
    "                    crim_det_copy['detail'] = x\n",
    "                    crim_det_list.append(crim_det_copy)\n",
    "        \n",
    "    return crim_det_list, crim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000Wall time: 6min 42s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crimdataset = []\n",
    "crimdetailset = []\n",
    "for i,urls in enumerate(dict_start.keys()):\n",
    "    page_data = dict_start[urls]\n",
    "    if (i%1000.0) == 0.0:\n",
    "        print i,\n",
    "    \n",
    "    election  = urls.split(\"/\")[3]\n",
    "    details, dataset = get_crimdata2(election,page_data,urls)\n",
    "    crimdataset = crimdataset + dataset\n",
    "    crimdetailset = crimdetailset + details\n",
    "\n",
    "with open(\"tempdata/dic_crim_state.json\",'w') as f:\n",
    "    json.dump(crimdataset,f)\n",
    "with open(\"tempdata/dic_crim_details_state.json\",'w') as f:\n",
    "    json.dump(crimdetailset,f)\n",
    "    \n",
    "criminal_df = pd.DataFrame(crimdataset)\n",
    "crim_details_df = pd.DataFrame(crimdetailset)\n",
    "criminal_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaner = lambda e: int(re.findall('\\d+', e.replace(',', '').split(\" ~ \")[0])[0])\n",
    "income_cols = [\"Relation\",\"PAN\",\"Year\",\"Income\"]\n",
    "\n",
    "def income_table(link):\n",
    "    page_candidate = dict_start[link]\n",
    "    c_soup = BeautifulSoup(page_candidate,\"html.parser\")\n",
    "    table_titles =[x.get_text().strip() for x in c_soup.findAll(\"h3\")]\n",
    "    tables = [x.find_next() for x in c_soup.findAll(\"h3\")]\n",
    "    dict_tab = dict(zip(table_titles,tables))\n",
    "    income_tab = dict_tab['Details of PAN and status of Income Tax return']\n",
    "    income_rows = income_tab.find_all(\"tr\")\n",
    "    dict_income = {}\n",
    "    df_inc = pd.DataFrame([])\n",
    "    if income_cols==[]:\n",
    "        dict_income = {'HH':{\"Year\":np.nan,\"PAN\":\"N\",\"Relation\":np.nan,\"Income\":np.nan}}\n",
    "    else:\n",
    "        for r in income_rows[1:]:\n",
    "            list_items = [x.get_text() for x in r.findAll(\"td\")]\n",
    "            if len(list_items)==4 and list_items[3]!=\"Nil\":\n",
    "                list_items[3] = cleaner(list_items[3])\n",
    "            if len(list_items)==4 and list_items[3]==\"Nil\":\n",
    "                list_items[3] = 0\n",
    "            dict_income[list_items[0]] = dict(zip(income_cols,list_items))\n",
    "        df_inc = df_inc.from_dict(dict_income,orient = \"index\")\n",
    "    try:\n",
    "        df_inc = df_inc[df_inc.PAN==\"Y\"]\n",
    "        HHinc = np.sum(df_inc['Income'])\n",
    "        HHDeclarations = np.count_nonzero(df_inc['PAN'])\n",
    "        self_income = dict_income['self']['Income']\n",
    "        self_declare = dict_income['self']['PAN']\n",
    "    except AttributeError:\n",
    "        df_inc=df_inc\n",
    "        HHinc = np.nan\n",
    "        HHDeclarations = 0\n",
    "        self_income = np.nan\n",
    "        self_declare = np.nan\n",
    "    newdict = {'self_inc':self_income,'self_declare':self_declare,'HHinc':HHinc,\"HHDeclarations\":HHDeclarations}\n",
    "    return newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000 4100 4200 4300 4400 4500 4600 4700 4800 4900 5000 5100 5200 5300 5400 5500 5600 5700 5800 5900 6000 6100 6200 6300 6400 6500 6600 6700 6800 6900 7000 7100 7200 7300 7400 7500 7600 7700 7800 7900 8000 8100 8200 8300 8400 8500 8600 8700 8800 8900 9000 9100 9200 9300 9400 9500 9600 9700 9800 9900 10000 10100 10200 10300 10400 10500 10600 10700 10800 10900 11000 11100 11200 11300 11400 11500 11600 11700 11800 11900 12000 12100 12200 12300 12400 12500 12600 12700 12800 12900 13000 13100 13200 13300 13400 13500 13600 13700 13800 13900 14000 14100 14200 14300 14400 14500 14600 14700 14800 14900 15000 15100 15200 15300 15400 15500 15600 15700 15800 15900 16000 16100 16200 16300 16400 16500 16600 16700 16800 16900 17000 17100 17200 17300 17400 17500 17600 17700 17800 17900 18000 18100 18200 18300 18400 18500 18600 18700 18800 18900 19000 19100 19200 19300 19400 19500 19600 19700 19800 19900 20000 20100 20200 20300 20400 20500 20600 20700 20800 20900 21000 21100 21200 21300 21400 21500 21600 21700 21800 21900 22000 22100 22200 22300 22400 22500 22600 22700 22800 22900 23000 23100 23200 23300 23400 23500 23600 23700 23800 23900 24000 24100 24200 24300 24400 24500 24600 24700 24800 24900 25000 25100 25200 25300 25400 25500 25600 25700 25800 25900 26000 26100 26200 26300 26400 26500 26600 26700 26800 26900 27000 27100 27200 27300 27400 27500 27600 27700 27800 27900 28000 28100 28200 28300 28400 28500 28600 28700 28800 28900 29000 29100 29200 29300 29400 29500 29600 29700 29800 29900 30000 30100 30200 30300 30400 30500 30600 30700 30800 30900 31000 31100 31200 31300 31400 31500 31600 31700 31800 31900 32000 32100 32200 32300 32400 32500 32600 32700 32800 32900 33000 33100 33200 33300 33400 33500 33600 33700 33800 33900 34000 34100 34200 34300 34400 34500 34600 34700 34800 34900 35000 35100 35200 35300 35400 35500 35600 35700 35800 35900 36000 36100 36200 36300 36400 36500 36600 36700 36800 36900 37000 37100 37200 37300 \n",
      " Number of errors:  0\n",
      "Wall time: 43min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counterror = 0\n",
    "dict_allinc = {}\n",
    "for k,url in enumerate(dict_start.keys()):\n",
    "    try:\n",
    "        dict_allinc[url] = income_table(url)\n",
    "    except TypeError:\n",
    "        counterror = counterror+1\n",
    "        print \"Error with this page: \", url\n",
    "    except KeyError:\n",
    "        counterror = counterror+1\n",
    "        print \"Error with this page: \", url\n",
    "    if k%1000==0:\n",
    "        print k,\n",
    "print \"\\n Number of errors: \", counterror\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_income = pd.DataFrame.from_dict(dict_allinc, orient = 'index')\n",
    "df_income['link'] = df_income.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Now onto the complete info on previous election ###\n",
    "ROOT_LINK = \"http://myneta.info\"\n",
    "def get_otherelec_link(page_text, url):\n",
    "    for a in pq(page_text)(\"a\"):\n",
    "        if a.text == \"Click here for more details\":\n",
    "            other_elec_link = pq(a).attr.href\n",
    "            return ROOT_LINK+other_elec_link\n",
    "    return False\n",
    "\n",
    "def get_otherelec_data(otherelec_link):\n",
    "    \n",
    "    otherelec_dict = {'common_link': otherelec_link}\n",
    "    \n",
    "    html = requests.get(otherelec_link)\n",
    "    doc = pq(html.content)\n",
    "    \n",
    "    columns = []\n",
    "    all_dicts = []\n",
    "    add = 0\n",
    "    trs = doc('tr')\n",
    "    for tr in trs:\n",
    "        elec_dict = otherelec_dict.copy()\n",
    "        for th in pq(tr)('th'):\n",
    "            columns.append(pq(th).text().replace(\" \",\"_\"))\n",
    "            add = 0\n",
    "        for i,td in enumerate(pq(tr)('td')):\n",
    "            a = pq(td)('a')\n",
    "            if a:\n",
    "                elec_dict['elec_link'] = ROOT_LINK+a.attr.href\n",
    "                elec_dict[columns[i]] = a.text()\n",
    "            else:\n",
    "                try:\n",
    "                    if pq(td)('span') and i < 6:\n",
    "                        elec_dict[columns[i]] = pq(td)('span').text()\n",
    "                    else:\n",
    "                        elec_dict[columns[i]] = str(pq(td).contents()[0]).encode('utf-8').strip().replace(',','')\n",
    "                except:\n",
    "                    print \"\"\n",
    "                    print \"Skipping col %s for %s\" % (columns[i], elec_dict['common_link'])\n",
    "            add = 1\n",
    "            \n",
    "        if add == 1:\n",
    "            all_dicts.append(elec_dict)\n",
    "    \n",
    "    return all_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_elec():\n",
    "    all_elec_data = []   \n",
    "    counter = 0.0\n",
    "    for key, val in dict_start.iteritems():\n",
    "        thelink = get_otherelec_link(val,key)\n",
    "        counter += 1\n",
    "        if counter%1000 == 0.0:\n",
    "            print \".\",\n",
    "        if thelink:\n",
    "            all_elec_data = all_elec_data + get_otherelec_data(thelink)\n",
    "    \n",
    "    df = pd.DataFrame(all_elec_data)\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "all_elecs_df = get_all_elec()\n",
    "all_elecs_df.to_csv(\"tempdata/all_elecs_df_state.csv\",encodig='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Now let's merge everything and obtain a nice dataframe with:\n",
    "# Basic info (from the constituency page)\n",
    "# Asset info and criminal info as well as income info when available (from the candidate page)\n",
    "# Info about the past (from the \"previous election\" table\n",
    "#                      in the candidate page and the information in each of those links)\n",
    "\n",
    "df_main = pd.read_csv(\"tempdata\\df_all_regional_elections.csv\")\n",
    "df_main = df_main.merge(criminal_df, left_on = 'link', right_on = 'url', how = 'left')\n",
    "df_main = df_main.merge(df_assets, left_on = 'link', right_on = 'url', how = 'left')\n",
    "df_main = df_main.merge(df_income, on='link', how = 'left')\n",
    "#df_main = df_main.merge(all_elecs_df, left_on = 'link', right_on = 'url', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_states2014 = df_main.drop_duplicates(['Name','consistuency'])\n",
    "df_states2014.to_csv('tempdata/states2014_all',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37255, 46)\n"
     ]
    }
   ],
   "source": [
    "print df_states2014.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
