{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pyquery import PyQuery as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_cand = \"http://myneta.info/ls2014/index.php?action=summary&subAction=candidates_analyzed&sort=candidate#summary\"\n",
    "winners_crim = \"http://myneta.info/ls2014/index.php?action=summary&subAction=winner_crime&sort=candidate#summary\"\n",
    "source=requests.get(all_cand) # Modify here: winners works with smaller sample, all_cand with the whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree= BeautifulSoup(source.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_links = tree.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_pol = tree.findAll('table')[2]\n",
    "rows = table_pol.findAll(\"tr\")[2:]\n",
    "print rows[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We build one dictionary per candidate #\n",
    "# We give each candidates to idenfiers: its order in the list and its url address\n",
    "\n",
    "def id2(r):\n",
    "    url_string = str(r.find(\"a\").get(\"href\"))\n",
    "    id2 = int(re.search(r'\\d+', url_string).group())\n",
    "    return id2\n",
    "\n",
    "def c_link(r):\n",
    "    return r.find(\"a\").get(\"href\")\n",
    "def name(r):\n",
    "    return r.find(\"a\").get_text()\n",
    "def cols(r):\n",
    "    return r.findAll(\"td\")\n",
    "def assets(r):\n",
    "    col = cols(r)\n",
    "    ass1 = col[6].get_text().split(\"~\")[0].encode('ascii', 'ignore').replace(\"Rs\",\"\").replace(\",\",\"\")\n",
    "    if ass1 == \"Nil\":\n",
    "        ass2 = 0\n",
    "    else:\n",
    "        ass2=int(ass1)\n",
    "    return ass2\n",
    "\n",
    "def liab(r):\n",
    "    col = cols(r)\n",
    "    liab1 = col[7].get_text().split(\"~\")[0].encode('ascii', 'ignore').replace(\"Rs\",\"\").replace(\",\",\"\")\n",
    "    if liab1 == \"Nil\":\n",
    "        liab2 = 0\n",
    "    else:\n",
    "        liab2 = int(liab1)\n",
    "    return liab2\n",
    "\n",
    "info_candidate = lambda r: [int(cols(r)[0].get_text()),id2(r), r.find(\"a\").get(\"href\"),r.find(\"a\").get_text(),\n",
    "                            cols(r)[2].get_text(),cols(r)[3].get_text(),cols(r)[5].get_text(),\n",
    "                            int(cols(r)[4].get_text()),assets(r), liab(r)]\n",
    "\n",
    "title = ['id','id2','url','name','district','party','education','nr_crime','assets','liabilities']\n",
    "dict_candidates = [dict(zip(title,info_candidate(r))) for r in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dict_candidates)\n",
    "dict_candidates[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we create a really big dictionary which stores url and page for each candidate\n",
    "# Work in progress...\n",
    "# First transform the thing into a dataframe\n",
    "\n",
    "df_pol = pd.DataFrame(dict_candidates)\n",
    "order_cols = ['id2','name','district','party','education','assets','liabilities','nr_crime','url']\n",
    "df_pol = df_pol[order_cols].sort(['assets'],ascending=0)\n",
    "\n",
    "df_small=df_pol[[\"id2\",\"url\"]][0:3] # I use df_small to test the code before running it on 8400 candidates!\n",
    "df_small\n",
    "df_pol.head()\n",
    "\n",
    "#Note already that the same guy can be candidate at 2 districts #\n",
    "# Might be a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlcache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    # Check if URL has already been visited.\n",
    "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "        time.sleep(1)\n",
    "        steps = len(urlcache)\n",
    "        if 100*int(steps/100)==steps:\n",
    "            print steps # This counter tells us how many links were downloaded at every 100 mark\n",
    "        # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n",
    "        # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n",
    "        try:\n",
    "            r = requests.get(\"http://myneta.info/ls2014/%s\" % url)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                urlcache[url] = r.text\n",
    "            else:\n",
    "                urlcache[url] = 1\n",
    "        except:\n",
    "            urlcache[url] = 2\n",
    "    return urlcache[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_pol[\"url\"].apply(get_page) # This is a very long call (~4.5 hours on full dataset)\n",
    "                              # I am saving it in order to run it only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.sum([(urlcache[k]==1) or (urlcache[k]==2) for k in urlcache])# no one or 0's\n",
    "print len(df_pol.url.unique())==len(urlcache)#we got all of the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open(\"tempdata/polinfo.json\",\"w\") as fd:\n",
    "#    json.dump(pol_pages, fd)\n",
    "#del urlcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"tempdata/polinfo.json\") as json_file:\n",
    "    pol_pages = json.load(json_file) # Next iterations will start from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pol_pages['candidate.php?candidate_id=6112'] = requests.get('http://myneta.info/ls2014/candidate.php?candidate_id=6112').text\n",
    "pol_pages['candidate.php?candidate_id=2923'] = requests.get('http://myneta.info/ls2014/candidate.php?candidate_id=2923').text\n",
    "pol_pages['candidate.php?candidate_id=8068'] = requests.get('http://myneta.info/ls2014/candidate.php?candidate_id=8068').text\n",
    "pol_pages['candidate.php?candidate_id=1469'] = requests.get('http://myneta.info/ls2014/candidate.php?candidate_id=1469').text\n",
    "pol_pages['candidate.php?candidate_id=5058'] = requests.get('http://myneta.info/ls2014/candidate.php?candidate_id=5058').text\n",
    "pol_pages['candidate.php?candidate_id=5390'] = requests.get('http://myneta.info/ls2014/candidate.php?candidate_id=5390').text\n",
    "pol_pages['candidate.php?candidate_id=4001'] = requests.get('http://myneta.info/ls2014/candidate.php?candidate_id=4001').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now each candidate has an url - so later we  will parse each of their pages and scrape them#\n",
    "#But first let's look how the page looks like for a few individual candidates\n",
    "pages_list = [pol_pages[x] for x in df_pol['url'][0:10]] # List of 10 that we can start looking at\n",
    "\n",
    "i = 3\n",
    "url_candidate = pol_pages.keys()[i]\n",
    "full_url = \"http://myneta.info/ls2014/\" + url_candidate\n",
    "page_candidate = pol_pages.values()[i]\n",
    "c_soup = BeautifulSoup(page_candidate,\"html.parser\")\n",
    "print full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Personal info #########\n",
    "\n",
    "personal = c_soup.findAll(attrs={\"class\": \"grid_3 alpha\"})[0]\n",
    "full_name = personal.find(\"h2\").get_text()\n",
    "district1 = personal.find(\"h5\").get_text()\n",
    "grid2 = personal.findAll(attrs={\"class\":\"grid_2 alpha\"})\n",
    "party_full = grid2[0].get_text().split(\":\")[1]\n",
    "age = int(grid2[2].get_text().split(\":\")[1])\n",
    "address = grid2[3].get_text().split(\":\")[1].split(\"\\n\")[1] # Careful this one changes\n",
    "self_profession = personal.find(\"p\").get_text().split('\\n')[0].split(\":\")[1]\n",
    "spouse_profession = personal.find(\"p\").get_text().split('\\n')[1].split(\":\")[1]\n",
    "print full_name, \"\\n works as a:\", self_profession, \". \\n Profession of spouse:\", spouse_profession, \". \\n Address: \",\n",
    "print address, \", in district: \", district1, \"for party:\", party_full, \"The dude is\", age, \"years old\"\n",
    "\n",
    "########## Assets  & Criminality ###########\n",
    "# Ok let's get serious and find his assets and criminal activity\n",
    "all_tables = c_soup.findAll(\"table\")\n",
    "print \"\\n \\n\" , len(all_tables)\n",
    "table_inc = all_tables[0]\n",
    "table_crim = all_tables[1]\n",
    "table_assets1 = all_tables[2]\n",
    "table_assets2 = all_tables[3]\n",
    "table_liab = all_tables[4]\n",
    "print table_inc\n",
    "# That does not really work: some have a table for criminal activities, others not\n",
    "# So we need more work to know exactly how to have the right table correspond to the right thing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## This code allows to get income levels of politicians and their relative #######\n",
    "######## There a few errors that remain (some pages do not go through - despite seeming valid) ######\n",
    "####### But possible to correct.\n",
    "\n",
    "cleaner = lambda e: int(re.findall('\\d+', e.replace(',', '').split(\" ~ \")[0])[0])\n",
    "income_cols = [\"Relation\",\"PAN\",\"Year\",\"Income\"]\n",
    "def income_table(candidate_id):\n",
    "    page_candidate = pol_pages[candidate_id]\n",
    "    c_soup = BeautifulSoup(page_candidate,\"html.parser\")\n",
    "    table_titles =[x.get_text().strip() for x in c_soup.findAll(\"h3\")]\n",
    "    tables = [x.find_next() for x in c_soup.findAll(\"h3\")]\n",
    "    dict_tab = dict(zip(table_titles,tables))\n",
    "    income_tab = dict_tab['Details of PAN and status of Income Tax return']\n",
    "    income_rows = income_tab.find_all(\"tr\")\n",
    "    dict_income = {}\n",
    "    df_inc = pd.DataFrame([])\n",
    "    if income_cols==[]:\n",
    "        dict_income = {'HH':{\"Year\":np.nan,\"PAN\":\"N\",\"Relation\":np.nan,\"Income\":np.nan}}\n",
    "    else:\n",
    "        for r in income_rows[1:]:\n",
    "            list_items = [x.get_text() for x in r.findAll(\"td\")]\n",
    "            if len(list_items)==4 and list_items[3]!=\"Nil\":\n",
    "                list_items[3] = cleaner(list_items[3])\n",
    "            if len(list_items)==4 and list_items[3]==\"Nil\":\n",
    "                list_items[3] = 0\n",
    "            dict_income[list_items[0]] = dict(zip(income_cols,list_items))\n",
    "        df_inc = df_inc.from_dict(dict_income,orient = \"index\")\n",
    "    try:\n",
    "        df_inc = df_inc[df_inc.PAN==\"Y\"]\n",
    "        HHinc = np.sum(df_inc['Income'])\n",
    "        HHDeclarations = np.count_nonzero(df_inc['PAN'])\n",
    "        self_income = dict_income['self']['Income']\n",
    "        self_declare = dict_income['self']['PAN']\n",
    "    except AttributeError:\n",
    "        df_inc=df_inc\n",
    "        HHinc = np.nan\n",
    "        HHDeclarations = 0\n",
    "        self_income = np.nan\n",
    "        self_declare = np.nan\n",
    "    newdict = {'self_inc':self_income,'self_declare':self_declare,'HHinc':HHinc,\"HHDeclarations\":HHDeclarations}\n",
    "    return newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "income_table(pol_pages.keys()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "counterror = 0\n",
    "dict_allinc = {}\n",
    "for k,cid in enumerate(pol_pages.keys()):\n",
    "    try:\n",
    "        dict_allinc[cid] = income_table(cid)\n",
    "    except TypeError:\n",
    "        counterror = counterror+1\n",
    "        print \"Error with this page: \", cid\n",
    "    if k%100==0:\n",
    "        print k,\n",
    "print \"Number of errors: \", counterror\n",
    "d_inc_HH = d_inc_fin.from_dict(dict_allinc,orient = \"index\") #d_inc_HH associates income to all candidates\n",
    "                                                             # as well as for the whole family\n",
    "                                                             # and the number of declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_inc_HH.to_csv(\"C:\\Users\\mkkes_000\\Dropbox\\Indiastuff\\incomes.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_inc = d_inc_HH.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_inc['ln_HHinc'] = np.log(d_inc['HHinc'])\n",
    "d_inc['ln_selfinc'] = np.log(d_inc['self_inc'])\n",
    "d_inc['sh_self'] = d_inc['self_inc']/d_inc['HHinc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(d_inc.HHinc)\n",
    "sns.kdeplot(d_inc.self_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
