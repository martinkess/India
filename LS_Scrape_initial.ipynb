{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pyquery import PyQuery as pq"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This notebook initiates our work. We scrape the election commission website at myneta.info, focusing first on Lok Sabha 2014, but we will extend it to all years and all state at later stages. We can use this initial page to do all of that. Note that this will be the \"clean\" programming sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our source page: Lok Sabha 2014#\n",
    "all_cand = \"http://myneta.info/ls2014/index.php?action=summary&subAction=candidates_analyzed&sort=candidate#summary\"\n",
    "source=requests.get(all_cand) # Modify here: winners works with smaller sample, all_cand with the whole set\n",
    "tree= BeautifulSoup(source.text,\"html.parser\")\n",
    "table_pol = tree.findAll('table')[2]\n",
    "rows = table_pol.findAll(\"tr\")[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8204\n"
     ]
    }
   ],
   "source": [
    "# We build one dictionary per candidate #\n",
    "# We give each candidates to idenfiers: its order in the list and its url address\n",
    "\n",
    "def id2(r):\n",
    "    url_string = str(r.find(\"a\").get(\"href\"))\n",
    "    id2 = int(re.search(r'\\d+', url_string).group())\n",
    "    return id2\n",
    "\n",
    "def c_link(r):\n",
    "    return r.find(\"a\").get(\"href\")\n",
    "def name(r):\n",
    "    return r.find(\"a\").get_text()\n",
    "def cols(r):\n",
    "    return r.findAll(\"td\")\n",
    "def assets(r):\n",
    "    col = cols(r)\n",
    "    ass1 = col[6].get_text().split(\"~\")[0].encode('ascii', 'ignore').replace(\"Rs\",\"\").replace(\",\",\"\")\n",
    "    if ass1 == \"Nil\":\n",
    "        ass2 = 0\n",
    "    else:\n",
    "        ass2=int(ass1)\n",
    "    return ass2\n",
    "\n",
    "def liab(r):\n",
    "    col = cols(r)\n",
    "    liab1 = col[7].get_text().split(\"~\")[0].encode('ascii', 'ignore').replace(\"Rs\",\"\").replace(\",\",\"\")\n",
    "    if liab1 == \"Nil\":\n",
    "        liab2 = 0\n",
    "    else:\n",
    "        liab2 = int(liab1)\n",
    "    return liab2\n",
    "\n",
    "info_candidate = lambda r: [int(cols(r)[0].get_text()),id2(r), r.find(\"a\").get(\"href\"),r.find(\"a\").get_text(),\n",
    "                            cols(r)[2].get_text(),cols(r)[3].get_text(),cols(r)[5].get_text(),\n",
    "                            int(cols(r)[4].get_text()),assets(r), liab(r)]\n",
    "\n",
    "title = ['id','id2','url','name','district','party','education','nr_crime','assets','liabilities']\n",
    "dict_candidates = [dict(zip(title,info_candidate(r))) for r in rows]\n",
    "print len(dict_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we create a really big dictionary which stores url and page for each candidate\n",
    "# Work in progress...\n",
    "# First transform dict_candidate into a dataframe\n",
    "\n",
    "df_pol = pd.DataFrame(dict_candidates)\n",
    "df_pol.to_csv(\"C:\\Users\\mkkes_000\\Dropbox\\Indiastuff\\OutputTables\\df_pol_LS2014.csv\", index = True)\n",
    "order_cols = ['id2','name','district','party','education','assets','liabilities','nr_crime','url']\n",
    "df_pol = df_pol[order_cols].sort(['assets'],ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlcache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    # Check if URL has already been visited.\n",
    "    url_error = []\n",
    "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "        time.sleep(1)\n",
    "        steps = len(urlcache)\n",
    "        if 100*int(steps/100)==steps:\n",
    "            print steps # This counter tells us how many links were downloaded at every 100 mark\n",
    "        # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n",
    "        # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n",
    "        try:\n",
    "            r = requests.get(\"http://myneta.info/ls2014/%s\" % url)\n",
    "            if r.status_code == 200:\n",
    "                urlcache[url] = r.text\n",
    "            else:\n",
    "                urlcache[url] = 1\n",
    "        except:\n",
    "            urlcache[url] = 2\n",
    "            url_error.append(url)\n",
    "            print url\n",
    "    return urlcache[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retry downloading missing pages:\n",
    "for r in url_error:\n",
    "    urlcache[r] = requests.get(\"http://myneta.info/ls2014/%s\" % r).text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_pol[\"url\"].apply(get_page) # This is a very long call (~4.5 hours on full dataset)\n",
    "                              # I am saving it in order to run it only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print np.sum([(urlcache[k]==1) or (urlcache[k]==2) for k in urlcache])# no one or 0's\n",
    "print len(df_pol.url.unique())==len(urlcache)#we got all of the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"tempdata/polinfo.json\",\"w\") as fd:\n",
    "    json.dump(pol_pages, fd)\n",
    "del urlcache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
